{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook, we will use ray tune to tune the hyperparameters of xgboost\n",
    "# we first download a sample of data for classification and break it into train and test\n",
    "# then we define the search space for ray tune hyperparameter assuming we are using a sklearn pipeline that had this steps:\n",
    "## 1. impute missing values\n",
    "## 2. scale the data\n",
    "## 3. encode categorical features\n",
    "## 4. train the model\n",
    "# we will use the tune.run() function to run the hyperparameter search\n",
    "# we will use the tune.track.log() function to log the metrics we want to optimize\n",
    "# we will use the tune.report() function to report the metrics we want to optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import the libraries we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import ray\n",
    "from ray import tune\n",
    "# from ray.tune import track\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's download a free dataset from kaggle to use for classification\n",
    "# the dataset is about predicting if a customer will churn or not\n",
    "# we will use the telecom customer churn dataset\n",
    "\n",
    "data_df = pd.read_csv(\"../data/churn-bigml-80.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not we split the data into train and test\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Area code', 'International plan']\n",
    "numerical = ['Total day minutes', 'Total day calls', 'Total day charge', 'Total eve minutes', 'Total eve calls',\n",
    "                'Total eve charge', 'Total night minutes', 'Total night calls', 'Total night charge', 'Total intl minutes',\n",
    "                'Total intl calls', 'Total intl charge', 'Customer service calls', 'Number vmail messages', 'Account length', ]\n",
    "\n",
    "label = 'Churn'\n",
    "\n",
    "\n",
    "X_train = train_df.drop(label, axis=1)\n",
    "y_train = train_df[label].apply(lambda x: 1 if x == 'True.' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop(label, axis=1)\n",
    "y_test = test_df[label].apply(lambda x: 1 if x == 'True.' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first define the search space for the transformers\n",
    "\n",
    "transformer_space = {\n",
    "    'simple_imputer__strategy': tune.choice(['mean', 'median', 'most_frequent']),\n",
    "    'standard_scaler__with_mean': tune.choice([True, False]),\n",
    "    'standard_scaler__with_std': tune.choice([True, False]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define the search space for the model, which will be xgboost\n",
    "# we will use the tune.choice() function to define the search space for each hyperparameter\n",
    "\n",
    "model_space = {\n",
    "    'n_estimators': tune.choice([100, 200, 300, 400, 500]),\n",
    "    'max_depth': tune.choice([3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "    'learning_rate': tune.choice([0.01, 0.05, 0.1, 0.15, 0.2]),\n",
    "    'gamma': tune.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    'min_child_weight': tune.choice([0, 1, 2, 3, 4, 5]),\n",
    "    'subsample': tune.choice([0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "    'colsample_bytree': tune.choice([0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "    'colsample_bylevel': tune.choice([0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "    'colsample_bynode': tune.choice([0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "    'reg_alpha': tune.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    'reg_lambda': tune.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    'scale_pos_weight': tune.choice([1, 2, 3, 4, 5]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformer(config):\n",
    "    return ColumnTransformer([\n",
    "        (\n",
    "            'one_hot_encoder',\n",
    "            OneHotEncoder(handle_unknown='ignore'),\n",
    "            categories\n",
    "            ),\n",
    "        (\n",
    "            'simple_imputer',\n",
    "            SimpleImputer(strategy=config['transformer']['simple_imputer__strategy']),\n",
    "            numerical\n",
    "        ),\n",
    "        (\n",
    "        'standard_scaler',\n",
    "        StandardScaler(\n",
    "            with_mean=config['transformer']['standard_scaler__with_mean'], \n",
    "            with_std=config['transformer']['standard_scaler__with_std']\n",
    "            ),\n",
    "        numerical\n",
    "        ),\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "def make_classifier(config):\n",
    "    return xgb.XGBClassifier(\n",
    "        n_estimators=config['model']['n_estimators'],\n",
    "        max_depth=config['model']['max_depth'],\n",
    "        learning_rate=config['model']['learning_rate'],\n",
    "        gamma=config['model']['gamma'],\n",
    "        min_child_weight=config['model']['min_child_weight'],\n",
    "        subsample=config['model']['subsample'],\n",
    "        colsample_bytree=config['model']['colsample_bytree'],\n",
    "        colsample_bylevel=config['model']['colsample_bylevel'],\n",
    "        colsample_bynode=config['model']['colsample_bynode'],\n",
    "        reg_alpha=config['model']['reg_alpha'],\n",
    "        reg_lambda=config['model']['reg_lambda'],\n",
    "        scale_pos_weight=config['model']['scale_pos_weight']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(config):\n",
    "    # Create transformer with given configuration\n",
    "    preprocessing = make_transformer(config)\n",
    "\n",
    "    # Create and train model with given configuration\n",
    "    model = make_classifier(config['model'])\n",
    "\n",
    "    # Combine preprocessing and model into a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "\n",
    "    # Send the accuracy to Tune to track the performance of this set of hyperparameters\n",
    "    tune.report(accuracy=acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    \"transformer\": transformer_space,\n",
    "    \"model\": model_space\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_model, \n",
    "    config=config, \n",
    "    num_samples=10,  # number of times to sample from the configuration space\n",
    "    resources_per_trial={\"cpu\": 4}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raytune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
